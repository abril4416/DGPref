VQA_PATH : /mnt/data1/rui/Rui_Data_Space/VQA
DATA_NAME : vsg
GEN_DATASETS : vsg
HAL_DATASETS : objhal,mmhal,amber
MODEL_NAME : Qwen/Qwen-VL-Chat
SEED : 1111
SAVE_NUM : 113
INFERENCE_BATCH : 12
NEG_WORD : Yes
POS_WORD : No
MAX_LEN : 100
CONSIDERED_ASPECTS : object,object,count,attribute,spatial,scale,text
PUNISH_SHORT : False
HALF_VSG : False
USE_BOTH : False
dpo_training : True
dpo_use_average : False
dpo_token_weighted : False
dpo_token_weight : 1.0
dpo_beta : 0.1
load_8bit : False
batch_size : 8
infer_batch_size : 12
micro_batch_size : 1
logging_steps : 68
save_total_limit : 5
warmup_steps : 100
num_epochs : 1
learning_rate : 7e-06
cutoff_len : 256
use_gradient_checkpointing : False
group_by_length : False
fp16 : True
init_from_checkpoint : False
lora_checkpoint_dir : /mnt/data1/rui/LoRA/Modules
lora_checkpoint_file : checkpoint-60300
lora_r : 32
lora_alpha : 32
lora_dropout : 0.05
target_modules : .*language_model.*\.(q_proj|v_proj)
temperature : 0.0
top_p : 0.75
top_k : 40
num_beams : 4
max_new_tokens : 1024
eval_step : 2400
gen_step : 2400
gen_start : 4800
val_set_size : 500
save_step : 500
output_dir : /mnt/data1/rui/LoRA/Output
resume_from_checkpoint : None
DEBUG : False
GPT_ADV_VAL : False
GT_OBJ : False
PARTIAL_POSNEG : False
Length of training set: 51297, length of testing set: 365
Batch size: 8, Gradient accumulation: 8
Number of device: 1
Training batch size: 1
Total training batch size: 8
Total number of epochs: 1
	Length of train dataloader: 51297
	Number of iterations per epoch: 6412
	Number of instances: 51297
	Total number of iterations: 6412
	Accelerate: 1
Padding token id: 151643
Epoch: 1 out of 1
	Iterations in total 51297
	Iteration 68
	Loss: 1.41, learning rate: 0.000005
	Iteration 136
	Loss: 1.26, learning rate: 0.000007
	Iteration 204
	Loss: 1.18, learning rate: 0.000007
	Iteration 272
	Loss: 1.13, learning rate: 0.000007
	Iteration 340
	Loss: 1.18, learning rate: 0.000007
	Iteration 408
	Loss: 1.00, learning rate: 0.000007
	Iteration 476
	Loss: 0.82, learning rate: 0.000007
	Iteration 544
	Loss: 0.96, learning rate: 0.000007
	Iteration 612
	Loss: 0.81, learning rate: 0.000006
	Iteration 680
	Loss: 0.88, learning rate: 0.000006
	Iteration 748
	Loss: 0.79, learning rate: 0.000006
	Iteration 816
	Loss: 0.77, learning rate: 0.000006
	Iteration 884
	Loss: 0.82, learning rate: 0.000006
	Iteration 952
	Loss: 0.73, learning rate: 0.000006
	Iteration 1020
	Loss: 0.90, learning rate: 0.000006
	Iteration 1088
	Loss: 0.93, learning rate: 0.000006
	Iteration 1156
	Loss: 0.79, learning rate: 0.000006
	Iteration 1224
	Loss: 0.70, learning rate: 0.000006
	Iteration 1292
	Loss: 0.99, learning rate: 0.000006
	Iteration 1360
	Loss: 0.86, learning rate: 0.000006
	Iteration 1428
	Loss: 0.95, learning rate: 0.000006
	Iteration 1496
	Loss: 0.88, learning rate: 0.000005
	Iteration 1564
	Loss: 0.68, learning rate: 0.000005
	Iteration 1632
	Loss: 0.63, learning rate: 0.000005
	Iteration 1700
	Loss: 0.77, learning rate: 0.000005
	Iteration 1768
	Loss: 0.78, learning rate: 0.000005
	Iteration 1836
	Loss: 0.92, learning rate: 0.000005
	Iteration 1904
	Loss: 0.76, learning rate: 0.000005
	Iteration 1972
	Loss: 0.80, learning rate: 0.000005
	Iteration 2040
	Loss: 0.73, learning rate: 0.000005
	Iteration 2108
	Loss: 0.70, learning rate: 0.000005
	Iteration 2176
	Loss: 0.80, learning rate: 0.000005
	Iteration 2244
	Loss: 1.03, learning rate: 0.000005
	Iteration 2312
	Loss: 0.63, learning rate: 0.000005
	Iteration 2380
	Loss: 0.72, learning rate: 0.000004
Iteration 2400, evaluation...
	Eval loss: 0.72, runtime 121.77
	Iteration 2448
	Loss: 0.74, learning rate: 0.000004
	Iteration 2516
	Loss: 0.82, learning rate: 0.000004
	Iteration 2584
	Loss: 0.83, learning rate: 0.000004
	Iteration 2652
	Loss: 0.69, learning rate: 0.000004
	Iteration 2720
	Loss: 0.68, learning rate: 0.000004
	Iteration 2788
	Loss: 0.65, learning rate: 0.000004
	Iteration 2856
	Loss: 0.67, learning rate: 0.000004
	Iteration 2924
	Loss: 0.52, learning rate: 0.000004
	Iteration 2992
	Loss: 0.71, learning rate: 0.000004
	Iteration 3060
	Loss: 0.58, learning rate: 0.000004
	Iteration 3128
	Loss: 0.67, learning rate: 0.000004
	Iteration 3196
	Loss: 0.74, learning rate: 0.000004
	Iteration 3264
	Loss: 0.61, learning rate: 0.000003
	Iteration 3332
	Loss: 0.62, learning rate: 0.000003
	Iteration 3400
	Loss: 0.77, learning rate: 0.000003
	Iteration 3468
	Loss: 0.78, learning rate: 0.000003
	Iteration 3536
	Loss: 0.66, learning rate: 0.000003
	Iteration 3604
	Loss: 0.63, learning rate: 0.000003
	Iteration 3672
	Loss: 0.63, learning rate: 0.000003
	Iteration 3740
	Loss: 0.73, learning rate: 0.000003
	Iteration 3808
	Loss: 0.70, learning rate: 0.000003
	Iteration 3876
	Loss: 0.71, learning rate: 0.000003
	Iteration 3944
	Loss: 0.63, learning rate: 0.000003
	Iteration 4012
	Loss: 0.71, learning rate: 0.000003
	Iteration 4080
	Loss: 0.55, learning rate: 0.000003
	Iteration 4148
	Loss: 0.60, learning rate: 0.000003
	Iteration 4216
	Loss: 0.83, learning rate: 0.000002
	Iteration 4284
	Loss: 0.71, learning rate: 0.000002
	Iteration 4352
	Loss: 0.64, learning rate: 0.000002
	Iteration 4420
	Loss: 0.78, learning rate: 0.000002
	Iteration 4488
	Loss: 0.62, learning rate: 0.000002
	Iteration 4556
	Loss: 0.59, learning rate: 0.000002
	Iteration 4624
	Loss: 0.49, learning rate: 0.000002
	Iteration 4692
	Loss: 0.53, learning rate: 0.000002
	Iteration 4760
	Loss: 0.69, learning rate: 0.000002
Iteration 4800, evaluation...
	Eval loss: 0.67, runtime 121.92
Hallucination chair eval: objhal
	Chair_s 8.00, Chair_i 2.01
Hallucination chair eval: mmhal
	Chair_s 0.00, Chair_i 0.00
Hallucination chair eval: amber
	Chair_s 54.00, Chair_i 1.50 
	Iteration 4828
	Loss: 0.68, learning rate: 0.000002
	Iteration 4896
	Loss: 0.58, learning rate: 0.000002
	Iteration 4964
	Loss: 0.65, learning rate: 0.000002
	Iteration 5032
	Loss: 0.66, learning rate: 0.000002
	Iteration 5100
	Loss: 0.53, learning rate: 0.000001
	Iteration 5168
	Loss: 0.59, learning rate: 0.000001
	Iteration 5236
	Loss: 0.66, learning rate: 0.000001
	Iteration 5304
	Loss: 0.64, learning rate: 0.000001
	Iteration 5372
	Loss: 0.62, learning rate: 0.000001
	Iteration 5440
	Loss: 0.66, learning rate: 0.000001
	Iteration 5508
	Loss: 0.58, learning rate: 0.000001
	Iteration 5576
	Loss: 0.53, learning rate: 0.000001
	Iteration 5644
	Loss: 0.67, learning rate: 0.000001
	Iteration 5712
	Loss: 0.66, learning rate: 0.000001
	Iteration 5780
	Loss: 0.58, learning rate: 0.000001
	Iteration 5848
	Loss: 0.68, learning rate: 0.000001
	Iteration 5916
	Loss: 0.55, learning rate: 0.000001
	Iteration 5984
	Loss: 0.63, learning rate: 0.000000
	Iteration 6052
	Loss: 0.51, learning rate: 0.000000
	Iteration 6120
	Loss: 0.53, learning rate: 0.000000
	Iteration 6188
	Loss: 0.75, learning rate: 0.000000
	Iteration 6256
	Loss: 0.58, learning rate: 0.000000
	Iteration 6324
	Loss: 0.54, learning rate: 0.000000
	Iteration 6392
	Loss: 0.65, learning rate: 0.000000


Training completed. Do not forget to share your model on huggingface.co/models =)


Maximum epoch: 4800
	evaluation auc: 8.00, accuracy: 2.01
Time Consumption: 44899.22
